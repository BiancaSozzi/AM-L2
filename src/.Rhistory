source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
setwd("D:/Facultad/Aprendizaje de Maquinas/AM-L2/src")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10)
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
formula = class ~ .
eta=0.05
n.out = 4
n.hidden = 3
eps=1e-3
max.iter=10
max.iter
n.in <- ncol(train.set)
# matrices of weights of the neuronal network
wji <- matrix(runif(n.in * n.hidden, -.05, .05), n.hidden, n.in)
wkj <- matrix(runif(n.hidden * n.out, -.05, .05), n.out, n.hidden)
# matrices of derivates of error with respecto to weights
dx.wji <- matrix(0, n.hidden, n.in)
dx.wkj <- matrix(0, n.out, n.hidden)
# activations for the hidden layer
a.j <- rep(0, nrow(wji))
# values of function of activation for the hidden layer
z.j <- rep(0, nrow(wji))
# activations for the output layer
a.k <- rep(0, nrow(wkj))
# values of function of activation for the output layer, that represents the
# output of the neuronal network: y_k(x,w) = z.k
z.k <- rep(0, nrow(wkj))
# counter for number of iteration
n.iter <- 0
# each element this vector is error of a iteration of backpropagation
errors <- c(.Machine$double.xmax)
errores
errors
error <- 0
train.set
1:nrow(train.set)
m <- 1
train.set[m,]
append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0)
model.frame(formula, train.set[m,])
append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0)
train.set[m, as.character(formula[2])]
formula[2]
coding(train.set[m, as.character(formula[2])])
coding(train.set[0, as.character(formula[2])])
coding(train.set[3, as.character(formula[2])])
t <- coding(train.set[m, as.character(formula[2])])
t
length(x)
x <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0)
length(x)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
install.packages("e1071")
install.packages("e1071")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
run.private.tests()
run.private.tests()
run.private.tests()
run.private.tests()
data <- c(1.2, 2.4, 1.7723, 2.72)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
softmax(data, data[2])
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
t
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
dx.wkj <- as.matrix(z.k - t) %*% fi.j
fi.j <- t(as.matrix(z.j))
as.matrix(z.k - t) %*% fi.j
