TOL <<- 1e-4
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
setwd("D:/Facultad/Aprendizaje de Maquinas/AM-L2/src")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
else cat(style("[ERROR]",fg="red"));cat(style())
else cat("[ERROR]")
if (res) {cat(style("[OK]",fg="green"))}
if (res) cat(style("[OK]",fg="green"))
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
print.name.test <- function(num, name) {
cat(paste("#", num, sep=""),
paste("Testing", name))
}
run.private.tests()
train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10
train.set
formula
eta=0.05
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
set.seed(1)
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
formula = class ~ .
eta=0.05
n.out = 4
n.hidden = 3
eps=1e-3
max.iter=10
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
n.in <- ncol(train.set)
# matrices of weights of the neuronal network
wji <- matrix(runif(n.in * n.hidden, -.05, .05), n.hidden, n.in)
wkj <- matrix(runif(n.hidden * n.out, -.05, .05), n.out, n.hidden)
# matrices of derivates of error with respecto to weights
dx.wji <- matrix(0, n.hidden, n.in)
dx.wkj <- matrix(0, n.out, n.hidden)
# activations for the hidden layer
a.j <- rep(0, nrow(wji))
# values of function of activation for the hidden layer
z.j <- rep(0, nrow(wji))
# activations for the output layer
a.k <- rep(0, nrow(wkj))
# values of function of activation for the output layer, that represents the
# output of the neuronal network: y_k(x,w) = z.k
z.k <- rep(0, nrow(wkj))
# counter for number of iteration
n.iter <- 0
# each element this vector is error of a iteration of backpropagation
errors <- c(.Machine$double.xmax)
error <- 0
m <- 1
x <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0) #agrega un 1 al principio
t <- coding(train.set[m, as.character(formula[2])]) ## devuelve el valor binario de la clase de la fila
# propagate the input forward through the network
#######
for(j in 1:n.hidden){
for(i in 1:length(x)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.k[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
########
# propagate the errors backward through the network
#######
#PAG 14 APUNTE
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
# fi.i <- t(as.matrix)
########
# gradient descent
#
# save current weights
wji.old <- wji
wkj.old <- wkj
x <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0) #agrega un 1 al principio
t <- coding(train.set[m, as.character(formula[2])]) ## devuelve el valor binario de la clase de la fila
# propagate the input forward through the network
#######
for(j in 1:n.hidden){
for(i in 1:length(x)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.k[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z.j[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
########
# propagate the errors backward through the network
#######
#PAG 14 APUNTE
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
# fi.i <- t(as.matrix)
########
# gradient descent
#
# save current weights
wji.old <- wji
wkj.old <- wkj
dx.wkj <- as.matrix(z.k - t) %*% fi.j
dx.wkj
x <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0) #agrega un 1 al principio
t <- coding(train.set[m, as.character(formula[2])]) ## devuelve el valor binario de la clase de la fila
# propagate the input forward through the network
#######
for(j in 1:n.hidden){
for(i in 1:length(x)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.k[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z.j[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
########
# propagate the errors backward through the network
#######
#PAG 14 APUNTE
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
# fi.i <- t(as.matrix)
########
# gradient descent
#
# save current weights
wji.old <- wji
wkj.old <- wkj
dx.wkj
n.in <- ncol(train.set)
# matrices of weights of the neuronal network
wji <- matrix(runif(n.in * n.hidden, -.05, .05), n.hidden, n.in)
wkj <- matrix(runif(n.hidden * n.out, -.05, .05), n.out, n.hidden)
# matrices of derivates of error with respecto to weights
dx.wji <- matrix(0, n.hidden, n.in)
dx.wkj <- matrix(0, n.out, n.hidden)
# activations for the hidden layer
a.j <- rep(0, nrow(wji))
# values of function of activation for the hidden layer
z.j <- rep(0, nrow(wji))
# activations for the output layer
a.k <- rep(0, nrow(wkj))
# values of function of activation for the output layer, that represents the
# output of the neuronal network: y_k(x,w) = z.k
z.k <- rep(0, nrow(wkj))
# counter for number of iteration
n.iter <- 0
# each element this vector is error of a iteration of backpropagation
errors <- c(.Machine$double.xmax)
error <- 0
m <- 1
x <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0) #agrega un 1 al principio
t <- coding(train.set[m, as.character(formula[2])]) ## devuelve el valor binario de la clase de la fila
for(j in 1:n.hidden){
for(i in 1:length(x)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.k[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z.j[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
z.k
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
dx.wkj
fi.j
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
for(j in 1:n.hidden){
for(i in 1:length(x)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.j[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z.j[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
APUNTE
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
dx.wkj
fi.j <- t(as.matrix(z.j))
dk <- as.matrix(z.k - t)
dx.wkj <- dk %*% fi.j
dx.wkj
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
x.i <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0) #agrega un 1 al principio
t <- coding(train.set[m, as.character(formula[2])]) ## devuelve el valor binario de la clase de la fila
# propagate the input forward through the network
#######
for(j in 1:n.hidden){
for(i in 1:length(x.i)){
a.j[j] <- a.j[j] + wji[j,i] * x[i]
}
z.j[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out){
for(j in 1:n.hidden){
a.k[k] <- a.k[k] + wkj[k,j] * z.j[j]
}
z.k[k] <- softmax(a.k,a.k[k])
}
########
# propagate the errors backward through the network
#######
#PAG 14 APUNTE
fi.j <- t(as.matrix(z.j))
dk <- as.matrix(z.k - t)
dx.wkj <- dk %*% fi.j # derivada de E respecto de wkj
dj <-
dx.wij <- dj %*% x.i
dk
wkj
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
dh <- dx.sigmoid(a.j)
fi.k <- t(as.matrix(wkj))
sum.wkj_dk <- fi.k %*% dk
dj <- dh * sum.wkj_dk
dx.wij <- dj %*% x.i
dx.wij
dh
dh
a.j
error.function(z.k, t)
z.k
t
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
# compute the error of this iteration
error <- error + error.function(z.k, t)
error.function(z.k, t)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R', echo=TRUE)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10)
backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10)
backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=30)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=30)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
setwd("D:/Facultad/Aprendizaje de Maquinas/l1-AM/src")
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
## utilizamos el conjunto de testeo para clasificar
cla <- clasificar(test.set,formula=class ~ .,bp)
setwd("D:/Facultad/Aprendizaje de Maquinas/AM-L2/src")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
bp <- backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10)
## grafique los errores por cada iteraciÃ³n de backpropagation
#######
plot(1:bp$iters, bp$errors)
bp$iters
bp$errors
bp$errors[-1]
bp$errors
plot(1:bp$iters, bp$errors[-1])
plot(1:bp$iters, bp$errors[-1], "l")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
run.backpropagation.experiment()
plot(1:bp$iters, bp$errors[-1],"iteracion","error", "l")
plot(1:bp$iters, bp$errors[-1],xlab="iteracion",ylab="error", "l")
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/backpropagation.R')
run.backpropagation.experiment()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
source('D:/Facultad/Aprendizaje de Maquinas/AM-L2/src/publicTest.R')
run.private.tests()
