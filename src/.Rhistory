shiny::runApp('D:/UTN/AM/Presentaci√≥n')
setwd("D:/UTN/AM/AM-L2/src")
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
set.seed(1)
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
eta=0.05
n.out = 4
n.hidden = 3
eps=1e-3
max.iter=10
formula = class~.
n.in <- ncol(train.set)
# matrices of weights of the neuronal network
wji <- matrix(runif(n.in * n.hidden, -.05, .05), n.hidden, n.in)
wkj <- matrix(runif(n.hidden * n.out, -.05, .05), n.out, n.hidden)
# matrices of derivates of error with respecto to weights
# i: entrada, j: intermedia, k: salida
dx.wji <- matrix(0, n.hidden, n.in)
dx.wkj <- matrix(0, n.out, n.hidden)
# activations for the hidden layer
a.j <- rep(0, nrow(wji))
# values of function of activation for the hidden layer
z.j <- rep(0, nrow(wji))
# activations for the output layer
a.k <- rep(0, nrow(wkj))
# values of function of activation for the output layer, that represents the
# output of the neuronal network: y_k(x,w) = z.k
z.k <- rep(0, nrow(wkj))
# counter for number of iteration
n.iter <- 0
# each element this vector is error of a iteration of backpropagation
errors <- c(.Machine$double.xmax)
error <- 0
m <- 1
s
x.i <- append(as.matrix(model.frame(formula, train.set[m,])[-1]), 1, 0)
t <- coding(train.set[m, as.character(formula[2])])
# propagate the input forward through the network
#######
for(j in 1:n.hidden) {
for(i in 1:length(x.i)) {
a.j[j] <- a.j[j] + wji[j,i]*x.i[i]
}
z.j[j] <- sigmoid(a.j[j])
}
for(k in 1:n.out) {
for(j in 1:n.hidden) {
a.k[k] <- a.k[k] + wkj[k,j]*z.j[j]
}
z.k[k] <- softmax(a.j, a.k[k])
}
fi.j <- t(as.matrix(z.j))
dx.wkj <- as.matrix(z.k - t) %*% fi.j
########
# gradient descent
#
# save current weights
wji.old <- wji
wkj.old <- wkj
dx.wkj
wji.old
wkj.old
dx.wkj
fi.j
dx.wkj
fi.j <- t(as.matrix(z.j))
dk <- as.matrix(z.k - t)
dx.wkj <-  dk %*% fi.j
dx.wkj
x.i
dj
sum.wkj_dk
dj <- dh.aj * sum.wkj_dk
dk
wkj
wkj %*% dk
wkj
dk
wkj
wkj %*% dk
t(as.matrix(wkj))
fi.k <- t(as.matrix(wkj))
fi.k %*% dk
sum.wkj_dk <- fi.k %*% dk
sum.wkj_dk
dh.aj
a.j
dx.sigmoid(a.j)
2^2
a-j
a.j
x <- a.j
x
exp(-x) / ((1 + exp(-x)) ^ 2)
dx.sigmoid(a.j)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
dx.sigmoid(a.j)
dh <- dx.sigmoid(a.j)
fi.k <- t(as.matrix(wkj))
sum.wkj_dk <- fi.k %*% dk
dj <- dh * sum.wkj_dk
dx.wji <- dj %*% x.i
dx.wji
dx.wkj
dh
dx.wji
dx.wji
dh
a.j
dx.sigmoid(a.j)
dx.wkj
dx.wji
eta
eta * dx.wkj
eta * dx.wji
wkj.old - eta * dx.wkj
wkj <- wkj.old - eta * dx.wkj
wji <- wji.old - eta * dx.wji
wkj
wji
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
max.iter
eps
errors
errors
eps
error
error.function
t
z.k
error.function(z.k, t)
error
error.function(z.k, t)
z.k
t
error.function(z.k, t)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
error.function(z.k, t)
z.k
t
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
error.function(z.k, t)
error <- error + error.function(z.k, t)
error
error.function(z.k, t)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
error.function(z.k, t)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
errors <- append(errors, error)
errors
eps
max.iter
has.satisfied.condition
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 10
)
eps
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
# eps = 1e-3,
eps = 0.1,
max.iter = 10
)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 10
)
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 10
)
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 20
)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 20
)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 20
)
setwd("D:/UTN/AM/AM-L2/src")
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 20
)
splits$train
set.seed(1)
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <- data[1:datap,] # estoy limitando el entrenamiento a 10 ejemplos
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
bp <- backprop(train.set[],formula = class ~ .,eta=0.05, n.out = 4, n.hidden = 3, eps=1e-3, max.iter=10)
source('D:/UTN/AM/AM-L2/src/backpropagation.R')
run.backpropagation.experiment()
set.seed(1)
datap <- 10
## Read dataset
data <- read.dataset("../data/faces.csv")
data <-
data[1:datap, ] # estoy limitando el entrenamiento a 10 ejemplos
## Split for train and test sets
splits <- split.data(data, 0.3)
train.set <- splits$train
test.set <- splits$test
bp <-
backprop(
train.set[],
formula = class ~ .,
eta = 0.05,
n.out = 4,
n.hidden = 3,
eps = 1e-3,
max.iter = 10
)
